SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/kiper/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 74b1b23f-627b-41a2-bd62-d3dd0e9dd43e

Logging initialized using configuration in jar:file:/opt/hive/lib/hive-common-3.1.3.jar!/hive-log4j2.properties Async: true
Hive Session ID = 12fb2765-948f-42fc-89d9-18c226cc3f88
OK
Time taken: 0.845 seconds
OK
Time taken: 0.155 seconds
Query ID = kiper_20250825002522_16cb332c-fe84-445a-a2a5-f2f4d2bff6c1
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2025-08-25 00:25:26,816 Stage-1 map = 0%,  reduce = 0%
2025-08-25 00:25:27,847 Stage-1 map = 100%,  reduce = 0%
2025-08-25 00:25:28,854 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1280385711_0001
Moving data to directory hdfs://localhost:9001/user/hive/warehouse/eclog_analysis.db/user_sessions
MapReduce Jobs Launched: 
Stage-Stage-1:  HDFS Read: 227825030 HDFS Write: 54 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 7.136 seconds
OK
Time taken: 0.173 seconds
Query ID = kiper_20250825002529_6e30064b-eb9a-459a-8fb0-49017d0c2168
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2025-08-25 00:25:32,069 Stage-1 map = 0%,  reduce = 0%
2025-08-25 00:25:39,086 Stage-1 map = 100%,  reduce = 0%
2025-08-25 00:25:47,105 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1436929360_0002
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2025-08-25 00:25:48,662 Stage-2 map = 100%,  reduce = 0%
2025-08-25 00:25:54,238 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_local322840790_0003
Moving data to directory hdfs://localhost:9001/user/hive/warehouse/eclog_analysis.db/popular_pages
MapReduce Jobs Launched: 
Stage-Stage-1:  HDFS Read: 455650168 HDFS Write: 108 SUCCESS
Stage-Stage-2:  HDFS Read: 455650168 HDFS Write: 5014 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 25.445 seconds
OK
Time taken: 0.418 seconds
Query ID = kiper_20250825002555_9963aab1-8262-4bb4-bdea-55df1e170e17
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2025-08-25 00:25:57,050 Stage-1 map = 0%,  reduce = 0%
2025-08-25 00:26:24,254 Stage-1 map = 100%,  reduce = 0%
2025-08-25 00:26:25,259 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1075618365_0004
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2025-08-25 00:26:30,108 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_local152585403_0005
Moving data to directory hdfs://localhost:9001/user/hive/warehouse/eclog_analysis.db/anomaly_detection
MapReduce Jobs Launched: 
Stage-Stage-1:  HDFS Read: 683475370 HDFS Write: 9920 SUCCESS
Stage-Stage-2:  HDFS Read: 683475370 HDFS Write: 92058 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 34.857 seconds
OK
Time taken: 0.017 seconds
FAILED: SemanticException [Error 10004]: Line 21:15 Invalid table alias or column reference 'logdate': (possible column names are: ip_id, user_id, request_timestamp, http_method, uri, http_version, response_code, bytes, referrer, user_agent, hour, day_of_week, log_date, status_category, is_bot, country, resource_type, has_referrer)
